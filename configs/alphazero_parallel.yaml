# configs/alphazero_parallel.yaml
# AlphaZero training with parallel game generation
# Optimized for H100/H200 GPUs

network_config:
  hidden_dim: 256
  num_heads: 8
  num_layers: 4
  dropout: 0.1
  species_embedding_dim: 64

# MCTS configuration
num_simulations: 200        # MCTS simulations per move
c_puct: 1.5                 # Exploration constant
dirichlet_alpha: 0.3        # Dirichlet noise for exploration
dirichlet_epsilon: 0.25     # Noise mixing weight
temperature: 1.0            # Initial temperature
temperature_drop_move: 15   # Move after which temperature drops
final_temperature: 0.1      # Temperature after drop

# Parallel self-play (KEY FOR GPU UTILIZATION)
games_per_iteration: 256    # Total games per iteration
parallel_games: 64          # Games running simultaneously (batch MCTS)

# Training schedule
total_iterations: 2000
batch_size: 2048
epochs_per_iteration: 4

# Optimization (no entropy bonus - MCTS provides exploration)
learning_rate: 0.0001
weight_decay: 0.0001
max_grad_norm: 0.5
value_loss_weight: 1.0

# Checkpointing and evaluation
checkpoint_frequency: 50
eval_frequency: 25
eval_games_per_opponent: 200

# Learning rate schedule
lr_warmup_iterations: 10
lr_decay: "cosine"

# Replay buffer
buffer_size: 1000000
min_buffer_size: 10000

# Torch compile for faster inference
torch_compile: true
torch_compile_mode: "reduce-overhead"

# Tablebase integration (perfect endgame play)
tablebase_path: "data/endgame_4card_final.tb"
use_tablebase_values: true   # Use tablebase values as training targets
use_tablebase_play: true     # Use tablebase moves in endgame

# Misc
seed: 42
device: "cuda"
experiment_name: "alphazero_parallel"

# Evaluation opponents
eval_opponents:
  - random
  - heuristic
  - outcome_heuristic
