# Fast iteration configuration for testing and debugging
# Uses smaller batch sizes and fewer iterations for quick feedback

# Network architecture (smaller for faster training)
network_config:
  hidden_dim: 64
  num_heads: 2
  num_layers: 1
  dropout: 0.1
  species_embedding_dim: 16

# PPO algorithm parameters
ppo_config:
  learning_rate: 0.0003  # Lower LR for stability
  clip_epsilon: 0.2
  value_coef: 0.5
  entropy_coef: 0.01
  gamma: 0.99
  gae_lambda: 0.95
  ppo_epochs: 2
  minibatch_size: 32
  max_grad_norm: 0.5
  normalize_advantages: true
  clip_value: false

# Training schedule (short for testing)
total_iterations: 100
games_per_iteration: 64
checkpoint_frequency: 10
eval_frequency: 5

# Self-play settings
self_play_temperature: 1.0

# Learning rate schedule
lr_warmup_iterations: 5
lr_decay: "linear"

# Replay buffer
buffer_size: 10000
min_buffer_size: 500

# Gradient accumulation
gradient_accumulation_steps: 1

# Misc settings
seed: 42
device: "auto"
experiment_name: "beastybar_fast_test"
checkpoint_dir: "checkpoints"
log_frequency: 1
