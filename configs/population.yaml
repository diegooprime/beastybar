# configs/population.yaml
# Population Training Configuration
# Phase 4: No exploitable weaknesses
#
# This configuration implements population-based training where multiple agents
# co-evolve through self-play, with exploiter agents continuously probing for
# weaknesses. When an exploiter finds a successful strategy (achieves >60% win rate),
# it joins the main population, and its strategy gets "patched" into all agents.
#
# Key concepts:
# - Population: A diverse set of agents that play against each other
# - Exploiters: Agents specifically trained to find weaknesses in the population
# - ELO ratings: Track relative skill levels within the population
# - Culling: Remove weak agents to maintain population quality

# =============================================================================
# NETWORK CONFIGURATION
# =============================================================================
# Inherits architecture from superhuman phase - proven effective for Beasty Bar
network_config:
  hidden_dim: 256              # Transformer hidden dimension
  num_heads: 8                 # Multi-head attention heads
  num_layers: 4                # Transformer encoder layers
  dropout: 0.1                 # Dropout for regularization
  species_embedding_dim: 64    # Learned embeddings for animal species

# =============================================================================
# MCTS CONFIGURATION (for self-play)
# =============================================================================
# MCTS provides stronger training signal than pure policy rollouts
num_simulations: 200           # Simulations per move decision
c_puct: 1.5                    # Exploration constant (higher = more exploration)
dirichlet_alpha: 0.3           # Dirichlet noise parameter for root exploration
dirichlet_epsilon: 0.25        # Weight of Dirichlet noise (0 = no noise)

# =============================================================================
# POPULATION-SPECIFIC SETTINGS
# =============================================================================
population_config:
  # --- Population Size ---
  population_size: 8           # Number of agents in the main population
                               # Larger = more diversity but slower convergence
  max_exploiters: 2            # Maximum concurrent exploiter agents training
                               # Each exploiter searches for a different weakness

  # --- ELO Rating System ---
  initial_rating: 1500         # Starting ELO rating for new agents
  k_factor: 32                 # ELO K-factor (higher = faster rating changes)
                               # 32 is standard for new players in chess

  # --- Exploit-Patch Cycle ---
  # The core mechanism for eliminating weaknesses:
  # 1. Exploiter trains against population, seeking high win rate
  # 2. If successful (>60%), exploiter joins population
  # 3. Other agents learn to counter the exploit through continued training
  exploit_threshold: 0.60      # Win rate needed for exploiter to join population
  exploit_max_iterations: 500  # Max training iters before abandoning exploiter
                               # Prevents wasting compute on unpromising exploits
  exploit_eval_frequency: 50   # Evaluate exploiter progress every N iterations
  exploit_games_per_eval: 100  # Games to play for each exploiter evaluation

  # --- Population Management ---
  cull_threshold: 0.30         # Minimum win rate to remain in population
                               # Agents below this are removed
  cull_frequency: 1000         # Check for weak agents every N iterations
  diversity_bonus: 0.1         # ELO bonus for agents with unique play styles
                               # Encourages strategic diversity

  # --- Self-Play Tournament ---
  # Regular round-robin tournaments update ELO ratings
  tournament_frequency: 100    # Run tournament every N training iterations
  tournament_games: 50         # Games per matchup in each tournament

# =============================================================================
# TRAINING SCHEDULE
# =============================================================================
total_iterations: 5000         # Total training iterations
                               # Population training requires more iterations
                               # due to multi-agent dynamics
games_per_iteration: 8192      # Self-play games per iteration
                               # High volume for stable gradient estimates
batch_size: 2048               # Training batch size
epochs_per_iteration: 4        # Training epochs per batch of game data

# =============================================================================
# OPTIMIZATION
# =============================================================================
learning_rate: 0.0001          # Conservative LR for stable multi-agent learning
lr_warmup_iterations: 50       # Warmup iterations for learning rate
lr_decay: "cosine"             # Cosine annealing for smooth LR decay

# =============================================================================
# EVALUATION
# =============================================================================
# Regular evaluation against fixed opponents tracks absolute skill
eval_frequency: 100            # Evaluate every N iterations
eval_games_per_opponent: 200   # Games per evaluation opponent

# Evaluation opponents (from weakest to strongest)
eval_opponents:
  - random                     # Baseline: random legal moves
  - heuristic                  # Rule-based heuristic agent
  - outcome_heuristic          # Forward-looking heuristic with outcome simulation
  - mcts-100                   # MCTS with 100 simulations (strong baseline)

# =============================================================================
# INFRASTRUCTURE
# =============================================================================
buffer_size: 2000000           # Replay buffer size (in transitions)
                               # Large buffer for population diversity
seed: 42                       # Random seed for reproducibility
device: "auto"                 # "auto", "cuda", "mps", or "cpu"
experiment_name: "population_phase4"
checkpoint_dir: "checkpoints/population"
checkpoint_frequency: 100      # Save checkpoints every N iterations

# =============================================================================
# PERFORMANCE OPTIMIZATION
# =============================================================================
parallel_games: 64             # Games running simultaneously per GPU
                               # Maximizes GPU utilization during self-play
async_game_generation: true    # Generate games asynchronously
async_num_workers: 16          # Worker processes for game generation
torch_compile: true            # Enable torch.compile() for 20-40% speedup
