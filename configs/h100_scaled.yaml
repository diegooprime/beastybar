# H100 Scaled Training Configuration
# Target: 1x H100 (80GB) on RunPod - Scaled up from h200_optimized
# Changes from h200_optimized:
#   - Larger network (hidden_dim 256, 8 heads, 4 layers, embedding 64)
#   - Larger batches (512 minibatch, 2048 games/iter)
#   - More parallelism (32 workers)
#   - Bigger replay buffer (400k capacity, 20k min)

# Network architecture (scaled up from h200_optimized)
network_config:
  hidden_dim: 256          # Up from 128 - larger capacity
  num_heads: 8             # Up from 4 - more attention heads
  num_layers: 4            # Up from 3 - deeper network
  dropout: 0.1
  species_embedding_dim: 64  # Up from 32 - richer embeddings

# PPO algorithm parameters (same proven settings from h200_optimized)
ppo_config:
  learning_rate: 0.0001    # Best stability from lr_low experiment
  clip_epsilon: 0.2
  value_coef: 0.5
  entropy_coef: 0.04       # Increased from 0.02 to prevent policy collapse
  gamma: 0.99
  gae_lambda: 0.95
  ppo_epochs: 4
  minibatch_size: 512      # Up from 256 - larger batches for bigger network
  max_grad_norm: 0.5
  normalize_advantages: true
  clip_value: true         # Prevent value function oscillation (stability fix)
  target_kl: 0.02          # Early stopping on KL divergence (stability fix)

# Training schedule - scaled up for more data throughput
total_iterations: 500
games_per_iteration: 2048  # Up from 512 - 4x more games per iteration
num_workers: 32            # Up from 16 - double parallelism
checkpoint_frequency: 25   # Save every 25 iterations
eval_frequency: 10         # Evaluate every 10 iters

# Self-play settings
self_play_temperature: 1.0
# Shaped rewards: Use score-margin based rewards instead of binary win/loss.
# When true, rewards scale with victory margin (1.0 + 0.1*margin, capped at 2.0).
# This helps credit assignment by providing more signal about quality of play.
shaped_rewards: false  # Enable for better credit assignment if training stalls

# Learning rate schedule
lr_warmup_iterations: 20   # ~3 minutes warmup
lr_decay: "cosine"         # Smoother convergence

# Replay buffer (scaled up for more games)
buffer_size: 400000        # Up from 100000 - 4x larger buffer
min_buffer_size: 20000     # Up from 5000 - 4x larger minimum

# Gradient accumulation
gradient_accumulation_steps: 1

# Evaluation settings
eval_games: 100            # More games for lower variance

# Misc settings
seed: 42
device: "cuda"             # Force CUDA for H100
experiment_name: "h100_scaled_v1"
checkpoint_dir: "checkpoints"
log_frequency: 1

# Model selection (save best by heuristic win rate)
save_best_model: true
best_model_metric: "heuristic"  # Track heuristic opponent performance
