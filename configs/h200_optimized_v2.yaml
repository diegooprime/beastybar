# H200 Optimized V2 - Max performance for 143GB VRAM
# Resuming from iter 99 checkpoint

network_config:
  hidden_dim: 256
  num_heads: 8
  num_layers: 4
  dropout: 0.1
  species_embedding_dim: 64

ppo_config:
  learning_rate: 0.0001
  clip_epsilon: 0.2
  value_coef: 0.5
  entropy_coef: 0.04
  gamma: 0.99
  gae_lambda: 0.95
  ppo_epochs: 4
  minibatch_size: 2048      # 4x larger (was 512)
  max_grad_norm: 0.5
  normalize_advantages: true
  clip_value: true
  target_kl: 0.02

# More games, more diverse opponents
total_iterations: 500
games_per_iteration: 4096   # 2x more (was 2048)
num_workers: 32
checkpoint_frequency: 25
eval_frequency: 10

# Better opponent diversity (less self-play, more heuristic)
use_opponent_pool: true
opponent_config:
  current_weight: 0.3       # Was 0.6 - less self-play
  checkpoint_weight: 0.3    # Was 0.2 - more league play
  random_weight: 0.1        # Same
  heuristic_weight: 0.3     # Was 0.1 - 3x more heuristic!
  max_checkpoints: 10

pool_checkpoint_frequency: 25

self_play_temperature: 1.0
shaped_rewards: false

lr_warmup_iterations: 20
lr_decay: "cosine"

buffer_size: 400000
min_buffer_size: 20000
gradient_accumulation_steps: 1
eval_games: 100

seed: 42
device: "cuda"
experiment_name: "ppo_h200_v2"
checkpoint_dir: "checkpoints"
log_frequency: 1

save_best_model: true
best_model_metric: "heuristic"

# torch.compile disabled - incompatible with checkpoint resume
torch_compile: false
torch_compile_mode: "reduce-overhead"
