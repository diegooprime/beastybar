# configs/alphazero_v2.yaml
# AlphaZero training with NetworkV2 architecture
# Phase 3: Asymmetric encoders, dueling architecture, auxiliary heads
# Target: ~3.5M parameters

# Network V2 configuration (asymmetric depths)
network_config:
  hidden_dim: 256
  num_heads: 8
  queue_layers: 6          # More capacity for order-sensitive queue
  bar_layers: 2            # Less for order-invariant zones
  hand_layers: 2
  fusion_layers: 4
  dropout: 0.1
  species_embedding_dim: 64
  use_dueling: true        # Enable dueling architecture
  use_auxiliary_heads: true # Enable multi-task learning
  auxiliary_weight: 0.1    # Weight for auxiliary losses

# Use V2 network architecture
network_version: "v2"

# MCTS configuration
num_simulations: 200        # MCTS simulations per move
c_puct: 1.5                 # Exploration constant
dirichlet_alpha: 0.3        # Dirichlet noise for exploration
dirichlet_epsilon: 0.25     # Noise mixing weight
temperature: 1.0            # Initial temperature
temperature_drop_move: 15   # Move after which temperature drops
final_temperature: 0.1      # Temperature after drop

# Parallel self-play (GPU batch efficiency)
games_per_iteration: 256    # Total games per iteration
parallel_games: 64          # Games running simultaneously

# Training schedule
total_iterations: 3000      # More iterations for V2 architecture
batch_size: 2048
epochs_per_iteration: 4

# Optimization (no entropy bonus - MCTS provides exploration)
learning_rate: 0.00008      # Slightly lower for larger network
weight_decay: 0.0001
max_grad_norm: 0.5
value_loss_weight: 1.0

# Checkpointing and evaluation
checkpoint_frequency: 50
eval_frequency: 25
eval_games_per_opponent: 200

# Learning rate schedule
lr_warmup_iterations: 20    # Longer warmup for larger network
lr_decay: "cosine"

# Replay buffer
buffer_size: 1000000
min_buffer_size: 10000

# Torch compile for faster inference
torch_compile: true
torch_compile_mode: "reduce-overhead"

# Tablebase integration (perfect endgame play)
tablebase_path: "data/endgame_4card_final.tb"
use_tablebase_values: true   # Use tablebase values as training targets
use_tablebase_play: true     # Use tablebase moves in endgame

# Misc
seed: 42
device: "cuda"
experiment_name: "alphazero_v2_phase3"

# Evaluation opponents
eval_opponents:
  - random
  - heuristic
  - aggressive
  - defensive
  - outcome_heuristic
