# configs/alphazero_v2_max.yaml
# AlphaZero V2 Maximum Configuration
# Target: 100% win rate against ALL opponents
# Network: 12.8M parameters with asymmetric encoders

# Network V2 configuration (full capacity)
network_config:
  hidden_dim: 256
  num_heads: 8
  queue_layers: 6          # 6 layers for order-critical queue
  bar_layers: 2            # 2 layers for order-invariant zones
  hand_layers: 2
  fusion_layers: 4
  dropout: 0.1
  species_embedding_dim: 64
  use_dueling: true        # Separate value and advantage streams
  use_auxiliary_heads: true # Queue position, score margin prediction
  auxiliary_weight: 0.1

# Use V2 network architecture
network_version: "v2"

# MCTS configuration (aggressive search)
num_simulations: 400        # More simulations for stronger policy targets
c_puct: 1.5                 # Exploration constant
dirichlet_alpha: 0.3        # Dirichlet noise for exploration
dirichlet_epsilon: 0.25     # Noise mixing weight
temperature: 1.0            # Initial temperature for diversity
temperature_drop_move: 12   # Drop temperature earlier for sharper play
final_temperature: 0.05     # Near-deterministic after drop

# Parallel self-play (maximize GPU utilization)
games_per_iteration: 512    # More games per iteration
parallel_games: 128         # Batch across more games for GPU efficiency

# Training schedule (extended for convergence)
total_iterations: 5000      # Train longer to reach 100% target
batch_size: 4096            # Larger batches for stable gradients
epochs_per_iteration: 4

# Optimization
learning_rate: 0.0001       # Standard AlphaZero LR
weight_decay: 0.0001
max_grad_norm: 0.5
value_loss_weight: 1.0
auxiliary_loss_weight: 0.1

# Checkpointing and evaluation
checkpoint_frequency: 100
eval_frequency: 50
eval_games_per_opponent: 200

# Learning rate schedule
lr_warmup_iterations: 50    # Longer warmup for stability
lr_decay: "cosine"

# Replay buffer (large for diversity)
buffer_size: 2000000        # 2M positions
min_buffer_size: 20000

# Performance optimization
torch_compile: true
torch_compile_mode: "reduce-overhead"

# Tablebase integration (perfect endgame)
tablebase_path: "data/endgame_4card_final.tb"
use_tablebase_values: true
use_tablebase_play: true

# Misc
seed: 42
device: "cuda"
experiment_name: "alphazero_v2_max"
checkpoint_dir: "checkpoints/alphazero_v2_max"

# === FULL OPPONENT SUITE ===
# Target: 100% win rate against all
eval_opponents:
  # Basic opponents (should be 99%+)
  - random

  # Heuristic variants (should be 95%+)
  - heuristic
  - aggressive
  - defensive
  - queue
  - skunk
  - noisy
  - online

  # Strong heuristics (should be 90%+)
  - outcome_heuristic
  - distilled_outcome

  # MCTS opponents (should be 85%+)
  - mcts-100
  - mcts-500

  # Past PPO models (should be 90%+)
  - "neural:checkpoints/v4/iter_949.pt"
  - "neural:checkpoints/92max.pt"
