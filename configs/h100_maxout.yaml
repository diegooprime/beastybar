# H100 MAXIMUM UTILIZATION CONFIG
# Target: 80%+ GPU utilization, 45%+ memory usage

network_config:
  hidden_dim: 256
  num_heads: 8
  queue_layers: 6
  bar_layers: 2
  hand_layers: 2
  fusion_layers: 4
  dropout: 0.1
  species_embedding_dim: 64
  use_dueling: true
  use_auxiliary_heads: true
  auxiliary_weight: 0.1

network_version: "v2"

# MCTS configuration - INCREASED for quality
num_simulations: 200          # Was 100 - better policy improvement
c_puct: 1.5
dirichlet_alpha: 0.3
dirichlet_epsilon: 0.25
temperature: 1.0
temperature_drop_move: 12
final_temperature: 0.1

# Parallel self-play - MASSIVELY INCREASED
games_per_iteration: 2048     # Was 512 - 4x more data per iteration
parallel_games: 512           # Was 128 - 4x more concurrent games

# Training schedule - ADJUSTED for larger batches
total_iterations: 1500        # Was 3000 - fewer iters, more data each
batch_size: 16384             # Was 4096 - 4x larger batches
epochs_per_iteration: 4

# Optimization
learning_rate: 0.0001
weight_decay: 0.0001
max_grad_norm: 0.5
value_loss_weight: 1.0
auxiliary_loss_weight: 0.1

# Checkpointing and evaluation
checkpoint_frequency: 50      # More frequent due to fewer iterations
eval_frequency: 25
eval_games_per_opponent: 200

# Learning rate schedule
lr_warmup_iterations: 15      # Adjusted for fewer iterations
lr_decay: "cosine"

# Replay buffer - INCREASED
buffer_size: 3000000          # Was 1500000 - 2x for more data
min_buffer_size: 30000        # Was 15000

# Performance - CRITICAL SETTINGS
torch_compile: true
torch_compile_mode: "reduce-overhead"  # Skips slow autotuning phase

# Tablebase
tablebase_path: "data/endgame_4card_final.tb"
use_tablebase_values: true
use_tablebase_play: true

# Misc
seed: 42
device: "cuda"
experiment_name: "h100_maxout"
checkpoint_dir: "checkpoints/h100_maxout"

# Evaluation opponents
eval_opponents:
  - random
  - heuristic
  - aggressive
  - defensive
  - mcts-100
  - mcts-500
