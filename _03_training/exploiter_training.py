"""Exploiter training for finding weaknesses in frozen target networks.

This module provides tools for training an exploiter network against a frozen
target network. The exploiter learns to find and exploit weaknesses in the
target's strategy, which is useful for:
- Robustness testing of trained models
- Identifying exploitable patterns in learned policies
- Generating diverse training data for model improvement

The exploiter training stops early if it achieves a win rate above the
configured threshold, indicating the target has exploitable weaknesses.

Example:
    from _03_training.exploiter_training import ExploiterTrainer, ExploiterConfig

    # Train an exploiter against a frozen checkpoint
    trainer = ExploiterTrainer(
        target_checkpoint=Path("checkpoints/model.pt"),
        config=ExploiterConfig(win_rate_threshold=0.70),
    )
    result = trainer.train()

    if result.converged:
        print(f"Found exploit! Win rate: {result.final_win_rate:.1%}")
"""

from __future__ import annotations

import logging
import time
from dataclasses import asdict, dataclass
from pathlib import Path
from typing import Any

import numpy as np

# Conditional imports for PyTorch
try:
    import torch
    import torch.nn as nn

    TORCH_AVAILABLE = True
except ImportError:
    TORCH_AVAILABLE = False
    torch = None
    nn = None

from _02_agents.neural.network import BeastyBarNetwork
from _02_agents.neural.utils import (
    NetworkConfig,
    get_device,
    load_network_from_checkpoint,
    seed_all,
)
from _03_training.ppo import (
    PPOBatch,
    compute_gae,
    entropy_bonus,
    iterate_minibatches,
    policy_loss,
    value_loss,
)
from _03_training.replay_buffer import ReplayBuffer, Transition
from _03_training.self_play import (
    generate_games,
    trajectories_to_transitions,
)
from _03_training.utils import inference_mode

logger = logging.getLogger(__name__)


def _ensure_torch() -> None:
    """Raise ImportError if PyTorch is not available."""
    if not TORCH_AVAILABLE:
        raise ImportError(
            "PyTorch is required for exploiter training. Install with: pip install torch"
        )


# ============================================================================
# Configuration
# ============================================================================


@dataclass
class ExploiterConfig:
    """Configuration for exploiter training.

    Attributes:
        max_iterations: Maximum number of training iterations.
        games_per_iteration: Number of games to generate per iteration.
        win_rate_threshold: Stop training if exploiter achieves this win rate.
            Default 0.70 means training stops when exploiter wins > 70%.
        learning_rate: Optimizer learning rate.
        batch_size: Minibatch size for PPO updates.
        ppo_epochs: Number of PPO epochs per training iteration.
        eval_interval: Evaluate exploiter every N iterations.
        eval_games: Number of games for each evaluation.
        gamma: Discount factor for future rewards.
        gae_lambda: Lambda for GAE bias-variance tradeoff.
        clip_epsilon: PPO clipping parameter.
        value_coef: Coefficient for value loss.
        entropy_coef: Coefficient for entropy bonus.
        max_grad_norm: Maximum gradient norm for clipping.
        temperature: Temperature for action sampling during game generation.
        seed: Random seed for reproducibility.
        buffer_size: Maximum size of replay buffer.
    """

    # Training control
    max_iterations: int = 500
    games_per_iteration: int = 256
    win_rate_threshold: float = 0.70

    # Optimizer settings
    learning_rate: float = 3e-4
    batch_size: int = 256

    # PPO settings
    ppo_epochs: int = 4
    gamma: float = 0.99
    gae_lambda: float = 0.95
    clip_epsilon: float = 0.2
    value_coef: float = 0.5
    entropy_coef: float = 0.01
    max_grad_norm: float = 0.5

    # Evaluation
    eval_interval: int = 50
    eval_games: int = 100

    # Game generation
    temperature: float = 1.0

    # Misc
    seed: int = 42
    buffer_size: int = 50_000

    def to_dict(self) -> dict[str, Any]:
        """Convert config to dictionary for serialization."""
        return asdict(self)

    @classmethod
    def from_dict(cls, data: dict[str, Any]) -> ExploiterConfig:
        """Create config from dictionary."""
        import dataclasses

        known_fields = {f.name for f in dataclasses.fields(cls)}
        filtered_data = {k: v for k, v in data.items() if k in known_fields}
        return cls(**filtered_data)

    def validate(self) -> None:
        """Validate configuration parameters.

        Raises:
            ValueError: If any parameter is invalid.
        """
        if self.max_iterations <= 0:
            raise ValueError(f"max_iterations must be positive, got {self.max_iterations}")
        if self.games_per_iteration <= 0:
            raise ValueError(
                f"games_per_iteration must be positive, got {self.games_per_iteration}"
            )
        if not 0.0 < self.win_rate_threshold <= 1.0:
            raise ValueError(
                f"win_rate_threshold must be in (0, 1], got {self.win_rate_threshold}"
            )
        if self.learning_rate <= 0:
            raise ValueError(f"learning_rate must be positive, got {self.learning_rate}")
        if self.batch_size <= 0:
            raise ValueError(f"batch_size must be positive, got {self.batch_size}")
        if self.eval_interval <= 0:
            raise ValueError(f"eval_interval must be positive, got {self.eval_interval}")


# ============================================================================
# Result Container
# ============================================================================


@dataclass
class ExploiterResult:
    """Result of exploiter training.

    Attributes:
        checkpoint_path: Path to the best exploiter checkpoint.
        final_win_rate: Win rate of the exploiter against the target.
        iterations_trained: Number of training iterations completed.
        target_checkpoint: Path to the frozen target checkpoint.
        converged: True if exploiter reached win_rate_threshold.
        training_time_seconds: Total training time in seconds.
        best_iteration: Iteration at which best checkpoint was saved.
    """

    checkpoint_path: Path
    final_win_rate: float
    iterations_trained: int
    target_checkpoint: Path
    converged: bool
    training_time_seconds: float = 0.0
    best_iteration: int = 0

    def __repr__(self) -> str:
        status = "CONVERGED" if self.converged else "INCOMPLETE"
        return (
            f"ExploiterResult({status}, "
            f"win_rate={self.final_win_rate:.1%}, "
            f"iterations={self.iterations_trained}, "
            f"time={self.training_time_seconds:.1f}s)"
        )


# ============================================================================
# Exploiter Trainer
# ============================================================================


class ExploiterTrainer:
    """Trains an exploiter network against a frozen target.

    The exploiter learns to find weaknesses in the target network's strategy.
    Training stops when the exploiter achieves > threshold win rate
    or max_iterations is reached.

    The exploiter always plays as Player 0 during training, and only its
    transitions are used for learning. The target network plays as Player 1
    with its weights frozen (no gradients).

    Attributes:
        config: Exploiter training configuration.
        target_checkpoint: Path to the frozen target checkpoint.
        exploiter: The exploiter network being trained.
        target: The frozen target network.
        optimizer: Adam optimizer for the exploiter.
        device: Device for training (CPU/GPU).

    Example:
        trainer = ExploiterTrainer(
            target_checkpoint=Path("checkpoints/model.pt"),
            config=ExploiterConfig(max_iterations=500),
        )
        result = trainer.train()
        print(f"Final win rate: {result.final_win_rate:.1%}")
    """

    def __init__(
        self,
        target_checkpoint: Path,
        config: ExploiterConfig | None = None,
        device: torch.device | None = None,
        output_dir: Path | None = None,
    ) -> None:
        """Initialize exploiter trainer.

        Args:
            target_checkpoint: Path to frozen target network checkpoint.
            config: Training configuration. Uses defaults if None.
            device: Device for training. Auto-detects if None.
            output_dir: Directory to save exploiter checkpoints.
                Defaults to same directory as target checkpoint.

        Raises:
            ImportError: If PyTorch is not available.
            FileNotFoundError: If target checkpoint does not exist.
            ValueError: If configuration is invalid.
        """
        _ensure_torch()

        if not target_checkpoint.exists():
            raise FileNotFoundError(f"Target checkpoint not found: {target_checkpoint}")

        self.config = config or ExploiterConfig()
        self.config.validate()
        self.target_checkpoint = Path(target_checkpoint)

        # Set random seeds
        seed_all(self.config.seed)

        # Determine device
        if device is None:
            self._device = get_device()
        else:
            self._device = device
        logger.info(f"Using device: {self._device}")

        # Load target network and freeze it
        self.target, self._network_config, _ = load_network_from_checkpoint(
            target_checkpoint, device=self._device
        )
        self.target.eval()
        self._freeze_network(self.target)
        logger.info(
            f"Loaded frozen target from {target_checkpoint} "
            f"({self.target.count_parameters():,} parameters)"
        )

        # Create fresh exploiter network with random initialization
        self.exploiter = BeastyBarNetwork(self._network_config).to(self._device)
        logger.info(
            f"Created exploiter network ({self.exploiter.count_parameters():,} parameters)"
        )

        # Create optimizer
        self.optimizer = torch.optim.Adam(
            self.exploiter.parameters(),
            lr=self.config.learning_rate,
        )

        # Create replay buffer
        self.replay_buffer = ReplayBuffer(
            max_size=self.config.buffer_size,
            observation_dim=self._network_config.observation_dim,
            action_dim=self._network_config.action_dim,
        )

        # Output directory for checkpoints
        if output_dir is None:
            self._output_dir = target_checkpoint.parent / "exploiter"
        else:
            self._output_dir = Path(output_dir)
        self._output_dir.mkdir(parents=True, exist_ok=True)

        # Training state
        self._iteration = 0
        self._best_win_rate = 0.0
        self._best_checkpoint_path: Path | None = None
        self._metrics_history: list[dict[str, float]] = []

    def _freeze_network(self, network: nn.Module) -> None:
        """Freeze all parameters in a network (disable gradients).

        Args:
            network: Network to freeze.
        """
        for param in network.parameters():
            param.requires_grad = False

    def _generate_games(self) -> tuple[list[Transition], list[list[Transition]]]:
        """Generate games between exploiter (P0) and frozen target (P1).

        Returns:
            Tuple of (all_transitions, trajectory_list) where:
            - all_transitions: All transitions from exploiter (P0)
            - trajectory_list: Separate lists per game for GAE computation
        """
        # Generate games with exploiter as P0, target as P1 (opponent_network)
        trajectories = generate_games(
            network=self.exploiter,
            num_games=self.config.games_per_iteration,
            temperature=self.config.temperature,
            device=self._device,
            opponent_network=self.target,
            use_vectorized=True,
        )

        # Extract only P0 (exploiter) transitions for training
        all_transitions = trajectories_to_transitions(trajectories, player=0)

        # Build trajectory list for GAE (each game's P0 transitions separately)
        trajectory_list = []
        for traj in trajectories:
            p0_transitions = [
                Transition(
                    observation=step.observation,
                    action_mask=step.action_mask,
                    action=step.action,
                    action_prob=step.action_prob,
                    value=step.value,
                    reward=step.reward,
                    done=step.done,
                )
                for step in traj.steps_p0
            ]
            if p0_transitions:
                trajectory_list.append(p0_transitions)

        return all_transitions, trajectory_list

    def _compute_win_rate(self, trajectories: list) -> float:
        """Compute exploiter (P0) win rate from trajectories.

        Args:
            trajectories: List of GameTrajectory objects.

        Returns:
            Win rate as fraction (0.0 to 1.0).
        """
        if not trajectories:
            return 0.0

        wins = sum(1 for t in trajectories if t.winner == 0)
        return wins / len(trajectories)

    def _train_on_buffer(
        self, trajectory_list: list[list[Transition]]
    ) -> dict[str, float]:
        """Train exploiter on collected experiences.

        Uses proper per-trajectory GAE computation.

        Args:
            trajectory_list: List of trajectory lists for GAE computation.

        Returns:
            Dictionary of training metrics.
        """
        if not trajectory_list:
            return {}

        # Compute GAE per trajectory
        all_observations = []
        all_actions = []
        all_action_probs = []
        all_values = []
        all_action_masks = []
        all_advantages = []
        all_returns = []

        for traj in trajectory_list:
            if not traj:
                continue

            # Extract arrays
            obs_arr = np.array([t.observation for t in traj], dtype=np.float32)
            acts_arr = np.array([t.action for t in traj], dtype=np.int64)
            probs_arr = np.array([t.action_prob for t in traj], dtype=np.float32)
            vals_arr = np.array([t.value for t in traj], dtype=np.float32)
            masks_arr = np.array([t.action_mask for t in traj], dtype=np.float32)
            rewards_arr = np.array([t.reward for t in traj], dtype=np.float32)
            dones_arr = np.array([t.done for t in traj], dtype=np.float32)

            # Compute GAE (bootstrap value is 0 since trajectory ends at termination)
            values_with_bootstrap = np.append(vals_arr, 0.0)
            advantages, returns = compute_gae(
                rewards=rewards_arr,
                values=values_with_bootstrap,
                dones=dones_arr,
                gamma=self.config.gamma,
                gae_lambda=self.config.gae_lambda,
            )

            all_observations.append(obs_arr)
            all_actions.append(acts_arr)
            all_action_probs.append(probs_arr)
            all_values.append(vals_arr)
            all_action_masks.append(masks_arr)
            all_advantages.append(advantages)
            all_returns.append(returns)

        if not all_observations:
            return {}

        # Concatenate all trajectories
        observations = np.concatenate(all_observations, axis=0)
        actions = np.concatenate(all_actions, axis=0)
        action_probs = np.concatenate(all_action_probs, axis=0)
        values = np.concatenate(all_values, axis=0)
        action_masks = np.concatenate(all_action_masks, axis=0)
        advantages = np.concatenate(all_advantages, axis=0)
        returns = np.concatenate(all_returns, axis=0)

        # Create PPOBatch
        ppo_batch = PPOBatch(
            observations=torch.from_numpy(observations).float().to(self._device),
            actions=torch.from_numpy(actions).long().to(self._device),
            old_log_probs=torch.log(
                torch.from_numpy(action_probs).float().clamp(min=1e-8)
            ).to(self._device),
            old_values=torch.from_numpy(values).float().to(self._device),
            action_masks=torch.from_numpy(action_masks).float().to(self._device),
            advantages=torch.from_numpy(advantages).float().to(self._device),
            returns=torch.from_numpy(returns).float().to(self._device),
        )

        # Training metrics accumulator
        metrics_accum: dict[str, float] = {
            "policy_loss": 0.0,
            "value_loss": 0.0,
            "entropy": 0.0,
            "total_loss": 0.0,
            "approx_kl": 0.0,
            "clip_fraction": 0.0,
        }
        num_updates = 0

        # Set exploiter to training mode
        self.exploiter.train()

        # PPO epochs
        for _epoch in range(self.config.ppo_epochs):
            for minibatch in iterate_minibatches(
                ppo_batch, self.config.batch_size, shuffle=True
            ):
                # Forward pass
                policy_logits, values_pred = self.exploiter(minibatch.observations)
                values_pred = values_pred.squeeze(-1)

                # Normalize advantages
                advantages_norm = minibatch.advantages
                advantages_norm = (advantages_norm - advantages_norm.mean()) / (
                    advantages_norm.std() + 1e-8
                )

                # Compute losses
                p_loss, approx_kl, clip_frac = policy_loss(
                    logits=policy_logits,
                    actions=minibatch.actions,
                    old_log_probs=minibatch.old_log_probs,
                    advantages=advantages_norm,
                    action_masks=minibatch.action_masks,
                    clip_epsilon=self.config.clip_epsilon,
                )

                v_loss = value_loss(
                    predicted_values=values_pred,
                    returns=minibatch.returns,
                    old_values=minibatch.old_values,
                    clip_epsilon=self.config.clip_epsilon,
                    clip_value=True,
                )

                ent_bonus = entropy_bonus(
                    logits=policy_logits,
                    action_masks=minibatch.action_masks,
                )

                # Combined loss
                total_loss = (
                    p_loss
                    + self.config.value_coef * v_loss
                    - self.config.entropy_coef * ent_bonus
                )

                # Optimization step
                self.optimizer.zero_grad()
                total_loss.backward()

                # Gradient clipping
                if self.config.max_grad_norm > 0:
                    torch.nn.utils.clip_grad_norm_(
                        self.exploiter.parameters(),
                        self.config.max_grad_norm,
                    )

                self.optimizer.step()

                # Accumulate metrics
                metrics_accum["policy_loss"] += p_loss.item()
                metrics_accum["value_loss"] += v_loss.item()
                metrics_accum["entropy"] += ent_bonus.item()
                metrics_accum["total_loss"] += total_loss.item()
                metrics_accum["approx_kl"] += approx_kl.item()
                metrics_accum["clip_fraction"] += clip_frac.item()
                num_updates += 1

        # Average metrics
        if num_updates > 0:
            for key in metrics_accum:
                metrics_accum[key] /= num_updates

        metrics_accum["num_updates"] = float(num_updates)
        metrics_accum["batch_size"] = float(len(ppo_batch))

        return metrics_accum

    def _save_checkpoint(self, win_rate: float) -> Path:
        """Save exploiter checkpoint.

        Args:
            win_rate: Current win rate for checkpoint naming.

        Returns:
            Path to saved checkpoint.
        """
        checkpoint_path = (
            self._output_dir / f"exploiter_iter_{self._iteration:06d}_wr{win_rate:.2f}.pt"
        )

        checkpoint = {
            "iteration": self._iteration,
            "model_state_dict": self.exploiter.state_dict(),
            "optimizer_state_dict": self.optimizer.state_dict(),
            "config": self.config.to_dict(),
            "network_config": self._network_config.to_dict(),
            "win_rate": win_rate,
            "target_checkpoint": str(self.target_checkpoint),
            "metrics_history": self._metrics_history,
        }

        torch.save(checkpoint, checkpoint_path, pickle_protocol=4)
        logger.info(f"Saved checkpoint: {checkpoint_path}")

        return checkpoint_path

    def evaluate(self) -> float:
        """Evaluate current exploiter win rate against target.

        Plays eval_games games with exploiter as P0 and computes win rate.

        Returns:
            Win rate as fraction (0.0 to 1.0).
        """
        with inference_mode(self.exploiter):
            trajectories = generate_games(
                network=self.exploiter,
                num_games=self.config.eval_games,
                temperature=0.1,  # Near-greedy for evaluation
                device=self._device,
                opponent_network=self.target,
                use_vectorized=True,
            )

        win_rate = self._compute_win_rate(trajectories)
        return win_rate

    def train_iteration(self) -> dict[str, float]:
        """Execute a single training iteration.

        One iteration:
        1. Generate games with exploiter vs target
        2. Train exploiter on collected transitions
        3. Optionally evaluate and save checkpoints

        Returns:
            Dictionary of metrics from this iteration.
        """
        iteration_start = time.time()
        metrics: dict[str, float] = {"iteration": float(self._iteration)}

        # Generate games
        gen_start = time.time()
        with inference_mode(self.exploiter):
            trajectories = generate_games(
                network=self.exploiter,
                num_games=self.config.games_per_iteration,
                temperature=self.config.temperature,
                device=self._device,
                opponent_network=self.target,
                use_vectorized=True,
            )

        gen_time = time.time() - gen_start

        # Compute generation win rate
        gen_win_rate = self._compute_win_rate(trajectories)

        # Extract transitions for training
        _, trajectory_list = self._generate_games_from_trajectories(trajectories)

        # Train on collected data
        train_start = time.time()
        train_metrics = self._train_on_buffer(trajectory_list)
        train_time = time.time() - train_start

        # Update metrics
        metrics.update(train_metrics)
        metrics["generation_win_rate"] = gen_win_rate
        metrics["generation_time"] = gen_time
        metrics["training_time"] = train_time
        metrics["games_generated"] = float(len(trajectories))
        metrics["total_transitions"] = float(
            sum(len(traj.steps_p0) for traj in trajectories)
        )

        iteration_time = time.time() - iteration_start
        metrics["iteration_time"] = iteration_time

        self._metrics_history.append(metrics)
        return metrics

    def _generate_games_from_trajectories(
        self, trajectories: list
    ) -> tuple[list[Transition], list[list[Transition]]]:
        """Extract P0 transitions from pre-generated trajectories.

        Args:
            trajectories: List of GameTrajectory objects.

        Returns:
            Tuple of (all_transitions, trajectory_list).
        """
        all_transitions = trajectories_to_transitions(trajectories, player=0)

        trajectory_list = []
        for traj in trajectories:
            p0_transitions = [
                Transition(
                    observation=step.observation,
                    action_mask=step.action_mask,
                    action=step.action,
                    action_prob=step.action_prob,
                    value=step.value,
                    reward=step.reward,
                    done=step.done,
                )
                for step in traj.steps_p0
            ]
            if p0_transitions:
                trajectory_list.append(p0_transitions)

        return all_transitions, trajectory_list

    def train(self) -> ExploiterResult:
        """Train exploiter until win rate threshold or max iterations.

        The training loop:
        1. Generates games between exploiter (P0) and frozen target (P1)
        2. Trains exploiter using PPO on its own transitions
        3. Periodically evaluates win rate and saves checkpoints
        4. Stops early if win rate exceeds threshold

        Returns:
            ExploiterResult with final checkpoint path, win rate, and metrics.
        """
        logger.info(
            f"Starting exploiter training against {self.target_checkpoint} "
            f"(max_iterations={self.config.max_iterations}, "
            f"threshold={self.config.win_rate_threshold:.0%})"
        )

        training_start = time.time()
        converged = False

        try:
            while self._iteration < self.config.max_iterations:
                # Training iteration
                metrics = self.train_iteration()

                # Periodic evaluation
                should_eval = (
                    self._iteration % self.config.eval_interval == 0
                    or self._iteration == 0
                )
                if should_eval:
                    eval_win_rate = self.evaluate()
                    metrics["eval_win_rate"] = eval_win_rate

                    logger.info(
                        f"Iteration {self._iteration}/{self.config.max_iterations} | "
                        f"Loss: {metrics.get('total_loss', 0.0):.4f} | "
                        f"Gen WR: {metrics.get('generation_win_rate', 0.0):.1%} | "
                        f"Eval WR: {eval_win_rate:.1%}"
                    )

                    # Save checkpoint if best
                    if eval_win_rate > self._best_win_rate:
                        self._best_win_rate = eval_win_rate
                        self._best_checkpoint_path = self._save_checkpoint(eval_win_rate)
                        logger.info(f"New best win rate: {eval_win_rate:.1%}")

                    # Check convergence
                    if eval_win_rate >= self.config.win_rate_threshold:
                        converged = True
                        logger.info(
                            f"Converged! Win rate {eval_win_rate:.1%} >= "
                            f"threshold {self.config.win_rate_threshold:.0%}"
                        )
                        break
                else:
                    # Log without eval
                    if self._iteration % 10 == 0:
                        logger.info(
                            f"Iteration {self._iteration}/{self.config.max_iterations} | "
                            f"Loss: {metrics.get('total_loss', 0.0):.4f} | "
                            f"Gen WR: {metrics.get('generation_win_rate', 0.0):.1%}"
                        )

                self._iteration += 1

        except KeyboardInterrupt:
            logger.info("Training interrupted by user")

        training_time = time.time() - training_start

        # Final evaluation if not just done
        if not converged and self._iteration % self.config.eval_interval != 0:
            final_win_rate = self.evaluate()
            if final_win_rate > self._best_win_rate:
                self._best_win_rate = final_win_rate
                self._best_checkpoint_path = self._save_checkpoint(final_win_rate)
        else:
            final_win_rate = self._best_win_rate

        # Ensure we have a checkpoint
        if self._best_checkpoint_path is None:
            self._best_checkpoint_path = self._save_checkpoint(final_win_rate)

        logger.info(
            f"Exploiter training complete: "
            f"iterations={self._iteration}, "
            f"win_rate={self._best_win_rate:.1%}, "
            f"converged={converged}, "
            f"time={training_time:.1f}s"
        )

        return ExploiterResult(
            checkpoint_path=self._best_checkpoint_path,
            final_win_rate=self._best_win_rate,
            iterations_trained=self._iteration,
            target_checkpoint=self.target_checkpoint,
            converged=converged,
            training_time_seconds=training_time,
            best_iteration=self._iteration,
        )


# ============================================================================
# Convenience Functions
# ============================================================================


def train_exploiter(
    target_checkpoint: str | Path,
    max_iterations: int = 500,
    win_rate_threshold: float = 0.70,
    output_dir: str | Path | None = None,
    device: str | None = None,
) -> ExploiterResult:
    """Convenience function to train an exploiter against a target.

    Args:
        target_checkpoint: Path to the frozen target checkpoint.
        max_iterations: Maximum training iterations.
        win_rate_threshold: Stop if exploiter wins > this fraction.
        output_dir: Directory for exploiter checkpoints.
        device: Device string ("cpu", "cuda", "mps") or None for auto.

    Returns:
        ExploiterResult with training outcome.

    Example:
        result = train_exploiter(
            "checkpoints/model.pt",
            max_iterations=500,
            win_rate_threshold=0.70,
        )
        if result.converged:
            print(f"Found exploit at {result.checkpoint_path}")
    """
    _ensure_torch()

    config = ExploiterConfig(
        max_iterations=max_iterations,
        win_rate_threshold=win_rate_threshold,
    )

    torch_device = None
    if device is not None:
        torch_device = torch.device(device)

    output_path = Path(output_dir) if output_dir else None

    trainer = ExploiterTrainer(
        target_checkpoint=Path(target_checkpoint),
        config=config,
        device=torch_device,
        output_dir=output_path,
    )

    return trainer.train()


def load_exploiter_from_checkpoint(
    checkpoint_path: str | Path,
    device: str | torch.device | None = None,
) -> tuple[BeastyBarNetwork, ExploiterConfig, float]:
    """Load a trained exploiter from checkpoint.

    Args:
        checkpoint_path: Path to exploiter checkpoint.
        device: Device to load model to.

    Returns:
        Tuple of (network, config, win_rate).

    Raises:
        FileNotFoundError: If checkpoint does not exist.
    """
    _ensure_torch()

    path = Path(checkpoint_path)
    if not path.exists():
        raise FileNotFoundError(f"Checkpoint not found: {path}")

    if device is None:
        device = get_device()
    elif isinstance(device, str):
        device = torch.device(device)

    checkpoint = torch.load(path, map_location=device, weights_only=False)

    # Extract config
    network_config = NetworkConfig.from_dict(checkpoint.get("network_config", {}))
    exploiter_config = ExploiterConfig.from_dict(checkpoint.get("config", {}))

    # Create and load network
    network = BeastyBarNetwork(network_config).to(device)
    network.load_state_dict(checkpoint["model_state_dict"])
    network.eval()

    win_rate = checkpoint.get("win_rate", 0.0)

    return network, exploiter_config, win_rate


__all__ = [
    "ExploiterConfig",
    "ExploiterResult",
    "ExploiterTrainer",
    "load_exploiter_from_checkpoint",
    "train_exploiter",
]
