"""Training infrastructure for Beasty Bar."""

from __future__ import annotations

from .checkpoint_manager import CheckpointManager
from .elo import EloRating, Leaderboard
from .evaluation import (
    EvaluationConfig,
    EvaluationResult,
    compare_agents,
    create_evaluation_report,
    create_opponent,
    estimate_elo,
    evaluate_agent,
    is_significantly_better,
    log_evaluation_results,
    wilson_confidence_interval,
)
from .exploit_patch_cycle import (
    CycleConfig,
    CycleRecord,
    ExploiterInfo,
    ExploitPatchManager,
)
from .game_generator import GameGenerator
from .opponent_pool import (
    CheckpointEntry,
    OpponentConfig,
    OpponentPool,
    OpponentType,
    SampledOpponent,
    create_opponent_network,
)
from .ppo import (
    TORCH_AVAILABLE,
    PolicyValueNetwork,
    PPOBatch,
    PPOConfig,
    RolloutData,
    compute_gae,
    entropy_bonus,
    iterate_minibatches,
    policy_loss,
    ppo_update,
    process_rollout,
    train_step,
    value_loss,
)
from .replay_buffer import Batch, ReplayBuffer, TrajectoryBoundary, Transition
from .self_play import (
    GameTrajectory,
    SelfPlayStats,
    compute_action_distribution,
    compute_stats,
    create_game_seeds,
    generate_games,
    generate_seed_sequence,
    play_game,
    set_self_play_seed,
    trajectories_to_transitions,
    trajectory_to_player_transitions,
)
from .tournament import MatchResult, Tournament, TournamentConfig
from .tracking import (
    ConsoleTracker,
    ExperimentTracker,
    WandbTracker,
    create_tracker,
    log_evaluation,
    log_self_play_stats,
    log_training_step,
)
from .trainer import (
    Trainer,
    TrainingConfig,
    create_trainer_from_checkpoint,
    get_learning_rate,
    load_training_checkpoint,
    save_training_checkpoint,
    set_learning_rate,
)
from .trajectory import (
    PendingStep,
    PPOPendingStep,
    PPOStep,
    TrajectoryStep,
)
from .utils import inference_mode, training_mode

__all__ = [
    "Batch",
    "CheckpointEntry",
    "CheckpointManager",
    "ConsoleTracker",
    "CycleConfig",
    "CycleRecord",
    "EloRating",
    "EvaluationConfig",
    "EvaluationResult",
    "ExperimentTracker",
    "ExploitPatchManager",
    "ExploiterInfo",
    "GameGenerator",
    "GameTrajectory",
    "Leaderboard",
    "MatchResult",
    "OpponentConfig",
    "OpponentPool",
    "OpponentType",
    "PPOBatch",
    "PPOConfig",
    "PPOPendingStep",
    "PPOStep",
    "PendingStep",
    "PolicyValueNetwork",
    "ReplayBuffer",
    "RolloutData",
    "SampledOpponent",
    "SelfPlayStats",
    "TORCH_AVAILABLE",
    "Tournament",
    "TournamentConfig",
    "Trainer",
    "TrainingConfig",
    "TrajectoryBoundary",
    "TrajectoryStep",
    "Transition",
    "WandbTracker",
    "compare_agents",
    "compute_action_distribution",
    "compute_gae",
    "compute_stats",
    "create_evaluation_report",
    "create_game_seeds",
    "create_opponent",
    "create_opponent_network",
    "create_tracker",
    "create_trainer_from_checkpoint",
    "entropy_bonus",
    "estimate_elo",
    "evaluate_agent",
    "generate_games",
    "generate_seed_sequence",
    "get_learning_rate",
    "inference_mode",
    "is_significantly_better",
    "iterate_minibatches",
    "load_training_checkpoint",
    "log_evaluation",
    "log_evaluation_results",
    "log_self_play_stats",
    "log_training_step",
    "play_game",
    "policy_loss",
    "ppo_update",
    "process_rollout",
    "save_training_checkpoint",
    "set_learning_rate",
    "set_self_play_seed",
    "train_step",
    "training_mode",
    "trajectories_to_transitions",
    "trajectory_to_player_transitions",
    "value_loss",
    "wilson_confidence_interval",
]
