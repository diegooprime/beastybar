"""Exploit-patch cycle manager for automated adversarial training.

This module implements the exploit-patch training paradigm:
1. Train main agent until plateau detected
2. Freeze main agent and train exploiter against it
3. Add exploiter to opponent pool
4. Continue main agent training against exploiter
5. Monitor until main agent overcomes exploiter, repeat

The exploit-patch cycle is a form of adversarial curriculum learning that
helps discover and fix weaknesses in the main agent's policy.

Example:
    from _03_training.exploit_patch_cycle import ExploitPatchManager, CycleConfig

    # Create manager with custom config
    config = CycleConfig(cycle_interval=200, plateau_threshold=0.02)
    manager = ExploitPatchManager(config=config, checkpoints_dir=Path("checkpoints"))

    # In training loop
    manager.record_win_rate(win_rate, iteration)
    if manager.should_start_cycle(iteration):
        exploiter = manager.run_cycle(main_checkpoint, device)
        # Add exploiter to opponent pool
"""

from __future__ import annotations

import logging
from dataclasses import dataclass
from pathlib import Path
from typing import Any

import numpy as np

# Conditional imports for PyTorch
try:
    import torch

    TORCH_AVAILABLE = True
except ImportError:
    TORCH_AVAILABLE = False
    torch = None


logger = logging.getLogger(__name__)


def _ensure_torch() -> None:
    """Raise ImportError if PyTorch is not available."""
    if not TORCH_AVAILABLE:
        raise ImportError("PyTorch is required for exploit-patch cycles. Install with: pip install torch")


# ============================================================================
# Data Classes
# ============================================================================


@dataclass
class CycleConfig:
    """Configuration for exploit-patch training cycles.

    Attributes:
        cycle_interval: Iterations between automatic cycles (if no plateau).
        plateau_window: Number of iterations to analyze for plateau detection.
        plateau_threshold: Win rate change threshold below which is considered plateau.
        max_exploiters: Maximum number of exploiters to keep in the pool.
        exploiter_weight: Weight for exploiters when sampling from opponent pool.
        exploiter_training_iterations: Number of training iterations for each exploiter.
        exploiter_games_per_iteration: Games per iteration when training exploiter.
        min_exploiter_win_rate: Minimum win rate for exploiter to be added to pool.
    """

    cycle_interval: int = 200
    plateau_window: int = 50
    plateau_threshold: float = 0.02
    max_exploiters: int = 10
    exploiter_weight: float = 0.20
    exploiter_training_iterations: int = 50
    exploiter_games_per_iteration: int = 128
    min_exploiter_win_rate: float = 0.55

    def __post_init__(self) -> None:
        """Validate configuration parameters."""
        if self.cycle_interval <= 0:
            raise ValueError(f"cycle_interval must be positive, got {self.cycle_interval}")
        if self.plateau_window <= 0:
            raise ValueError(f"plateau_window must be positive, got {self.plateau_window}")
        if not 0.0 < self.plateau_threshold < 1.0:
            raise ValueError(f"plateau_threshold must be in (0, 1), got {self.plateau_threshold}")
        if self.max_exploiters <= 0:
            raise ValueError(f"max_exploiters must be positive, got {self.max_exploiters}")
        if not 0.0 <= self.exploiter_weight <= 1.0:
            raise ValueError(f"exploiter_weight must be in [0, 1], got {self.exploiter_weight}")

    def to_dict(self) -> dict[str, Any]:
        """Convert to dictionary for serialization."""
        return {f: getattr(self, f) for f in self.__dataclass_fields__}

    @classmethod
    def from_dict(cls, data: dict[str, Any]) -> CycleConfig:
        """Create from dictionary."""
        return cls(**{k: v for k, v in data.items() if k in cls.__dataclass_fields__})


@dataclass
class ExploiterInfo:
    """Information about a trained exploiter.

    Attributes:
        checkpoint_path: Path to the exploiter's checkpoint file.
        trained_at_iteration: Main agent iteration when exploiter was trained.
        target_iteration: Main agent iteration that the exploiter targets.
        win_rate_vs_target: Exploiter's win rate against the target main agent.
        training_iterations: Number of iterations used to train the exploiter.
    """

    checkpoint_path: Path
    trained_at_iteration: int
    target_iteration: int
    win_rate_vs_target: float
    training_iterations: int = 0

    def to_dict(self) -> dict[str, Any]:
        """Convert to dictionary for serialization."""
        return {
            "checkpoint_path": str(self.checkpoint_path),
            "trained_at_iteration": self.trained_at_iteration,
            "target_iteration": self.target_iteration,
            "win_rate_vs_target": self.win_rate_vs_target,
            "training_iterations": self.training_iterations,
        }

    @classmethod
    def from_dict(cls, data: dict[str, Any]) -> ExploiterInfo:
        """Create from dictionary."""
        return cls(
            checkpoint_path=Path(data["checkpoint_path"]),
            trained_at_iteration=data["trained_at_iteration"],
            target_iteration=data["target_iteration"],
            win_rate_vs_target=data["win_rate_vs_target"],
            training_iterations=data.get("training_iterations", 0),
        )


@dataclass
class CycleRecord:
    """Record of a completed exploit-patch cycle.

    Attributes:
        cycle_number: Sequential number of this cycle.
        started_iteration: Main training iteration when cycle started.
        exploiter_info: Information about the trained exploiter.
        trigger: What triggered the cycle ("plateau" or "scheduled").
        completed_iteration: Iteration when cycle completed (exploiter trained).
        duration_seconds: Time taken to complete the cycle.
    """

    cycle_number: int
    started_iteration: int
    exploiter_info: ExploiterInfo
    trigger: str
    completed_iteration: int = 0
    duration_seconds: float = 0.0

    def to_dict(self) -> dict[str, Any]:
        """Convert to dictionary for serialization."""
        return {
            "cycle_number": self.cycle_number,
            "started_iteration": self.started_iteration,
            "exploiter_info": self.exploiter_info.to_dict(),
            "trigger": self.trigger,
            "completed_iteration": self.completed_iteration,
            "duration_seconds": self.duration_seconds,
        }

    @classmethod
    def from_dict(cls, data: dict[str, Any]) -> CycleRecord:
        """Create from dictionary."""
        return cls(
            cycle_number=data["cycle_number"],
            started_iteration=data["started_iteration"],
            exploiter_info=ExploiterInfo.from_dict(data["exploiter_info"]),
            trigger=data["trigger"],
            completed_iteration=data.get("completed_iteration", 0),
            duration_seconds=data.get("duration_seconds", 0.0),
        )


@dataclass
class WinRateRecord:
    """Record of a win rate measurement.

    Attributes:
        iteration: Training iteration when win rate was recorded.
        win_rate: The recorded win rate value.
    """

    iteration: int
    win_rate: float


# ============================================================================
# Exploit-Patch Manager
# ============================================================================


class ExploitPatchManager:
    """Manages automated exploit-patch training cycles.

    The exploit-patch cycle is a training paradigm that:
    1. Detects plateau in main agent training (based on win rate stagnation)
    2. Freezes main agent and trains an exploiter against it
    3. Adds successful exploiter to opponent pool
    4. Continues main agent training against the exploiter
    5. Monitors until main agent beats exploiter, then repeats

    This helps discover and patch weaknesses in the main agent's policy.

    Attributes:
        config: Configuration for cycle behavior.
        exploiters: List of trained exploiters.
        cycle_history: History of completed cycles.
        win_rate_history: Recent win rate records for plateau detection.
        checkpoints_dir: Directory for saving exploiter checkpoints.

    Example:
        manager = ExploitPatchManager(
            config=CycleConfig(cycle_interval=200),
            checkpoints_dir=Path("checkpoints/exploiters"),
        )

        # In training loop
        for iteration in range(total_iterations):
            # ... training step ...
            manager.record_win_rate(eval_win_rate, iteration)

            if manager.should_start_cycle(iteration):
                exploiter = manager.run_cycle(main_checkpoint, device)
                opponent_pool.add_exploiter(exploiter)
    """

    def __init__(
        self,
        config: CycleConfig | None = None,
        checkpoints_dir: Path | None = None,
    ) -> None:
        """Initialize the exploit-patch manager.

        Args:
            config: Cycle configuration. Uses defaults if None.
            checkpoints_dir: Directory for saving exploiter checkpoints.
                            Creates "exploiters" subdirectory if not provided.
        """
        self.config = config or CycleConfig()
        self.exploiters: list[ExploiterInfo] = []
        self.cycle_history: list[CycleRecord] = []
        self._win_rate_records: list[WinRateRecord] = []
        self._last_cycle_iteration: int = -self.config.cycle_interval  # Allow immediate first cycle

        # Setup checkpoints directory
        if checkpoints_dir is not None:
            self.checkpoints_dir = checkpoints_dir / "exploiters"
        else:
            self.checkpoints_dir = Path("checkpoints/exploiters")
        self.checkpoints_dir.mkdir(parents=True, exist_ok=True)

        logger.info(
            f"ExploitPatchManager initialized: "
            f"cycle_interval={self.config.cycle_interval}, "
            f"plateau_window={self.config.plateau_window}, "
            f"plateau_threshold={self.config.plateau_threshold:.2%}"
        )

    @property
    def win_rate_history(self) -> list[float]:
        """Get win rate values as a list for backward compatibility."""
        return [r.win_rate for r in self._win_rate_records]

    def record_win_rate(self, win_rate: float, iteration: int) -> None:
        """Record heuristic win rate for plateau detection.

        Should be called after each evaluation against the heuristic baseline.

        Args:
            win_rate: Win rate against heuristic opponent (0.0 to 1.0).
            iteration: Current training iteration.
        """
        self._win_rate_records.append(WinRateRecord(iteration=iteration, win_rate=win_rate))

        # Keep only records within the plateau window
        # Use iteration-based filtering for accuracy
        min_iteration = iteration - self.config.plateau_window * 2  # Keep some buffer
        self._win_rate_records = [
            r for r in self._win_rate_records if r.iteration >= min_iteration
        ]

        logger.debug(f"Recorded win rate {win_rate:.2%} at iteration {iteration}")

    def should_start_cycle(self, current_iteration: int) -> bool:
        """Check if an exploit-patch cycle should start.

        Returns True if:
        - Plateau detected (win rate stagnant for plateau_window iterations)
        - OR cycle_interval iterations have passed since last cycle

        Args:
            current_iteration: Current training iteration.

        Returns:
            True if a cycle should be started, False otherwise.
        """
        # Check scheduled interval
        iterations_since_last = current_iteration - self._last_cycle_iteration
        if iterations_since_last >= self.config.cycle_interval:
            logger.info(
                f"Scheduled cycle triggered at iteration {current_iteration} "
                f"({iterations_since_last} iterations since last cycle)"
            )
            return True

        # Check for plateau
        if self.detect_plateau():
            logger.info(f"Plateau detected at iteration {current_iteration}")
            return True

        return False

    def detect_plateau(self) -> bool:
        """Detect if training has plateaued based on recent win rates.

        A plateau is detected when the win rate change over the plateau_window
        is below the plateau_threshold.

        Returns:
            True if plateau detected, False otherwise.
        """
        if len(self._win_rate_records) < self.config.plateau_window:
            return False

        # Get win rates from the plateau window
        recent_records = self._win_rate_records[-self.config.plateau_window :]
        win_rates = [r.win_rate for r in recent_records]

        # Calculate change: compare first half to second half
        half = len(win_rates) // 2
        first_half_avg = np.mean(win_rates[:half])
        second_half_avg = np.mean(win_rates[half:])

        change = abs(second_half_avg - first_half_avg)

        if change < self.config.plateau_threshold:
            logger.debug(
                f"Plateau detected: change={change:.4f} < threshold={self.config.plateau_threshold:.4f} "
                f"(first_half={first_half_avg:.4f}, second_half={second_half_avg:.4f})"
            )
            return True

        return False

    def run_cycle(
        self,
        main_checkpoint: Path,
        device: torch.device,
        training_config: dict[str, Any] | None = None,
    ) -> ExploiterInfo:
        """Run one exploit-patch cycle.

        Trains an exploiter against the frozen main agent checkpoint.

        Args:
            main_checkpoint: Path to current main agent checkpoint.
            device: Device for exploiter training.
            training_config: Optional training configuration overrides.

        Returns:
            ExploiterInfo for the trained exploiter.

        Raises:
            ImportError: If PyTorch is not available.
            FileNotFoundError: If main_checkpoint does not exist.
        """
        import time

        _ensure_torch()

        if not main_checkpoint.exists():
            raise FileNotFoundError(f"Main checkpoint not found: {main_checkpoint}")

        start_time = time.time()
        cycle_number = len(self.cycle_history) + 1

        # Determine trigger type
        trigger = "scheduled"
        if self.detect_plateau():
            trigger = "plateau"

        logger.info(
            f"Starting exploit-patch cycle {cycle_number} "
            f"(trigger={trigger}, target={main_checkpoint})"
        )

        # Get current iteration from checkpoint
        current_iteration = self._get_checkpoint_iteration(main_checkpoint, device)

        # Train exploiter
        exploiter_checkpoint, win_rate = self._train_exploiter(
            target_checkpoint=main_checkpoint,
            device=device,
            cycle_number=cycle_number,
            training_config=training_config,
        )

        # Create exploiter info
        exploiter_info = ExploiterInfo(
            checkpoint_path=exploiter_checkpoint,
            trained_at_iteration=current_iteration,
            target_iteration=current_iteration,
            win_rate_vs_target=win_rate,
            training_iterations=self.config.exploiter_training_iterations,
        )

        # Add to exploiters list if successful
        if win_rate >= self.config.min_exploiter_win_rate:
            self._add_exploiter(exploiter_info)
            logger.info(
                f"Exploiter added to pool with win rate {win_rate:.2%} "
                f"(pool size: {len(self.exploiters)})"
            )
        else:
            logger.warning(
                f"Exploiter win rate {win_rate:.2%} below threshold "
                f"{self.config.min_exploiter_win_rate:.2%}, not added to pool"
            )

        # Record cycle
        duration = time.time() - start_time
        cycle_record = CycleRecord(
            cycle_number=cycle_number,
            started_iteration=current_iteration,
            exploiter_info=exploiter_info,
            trigger=trigger,
            completed_iteration=current_iteration,
            duration_seconds=duration,
        )
        self.cycle_history.append(cycle_record)
        self._last_cycle_iteration = current_iteration

        logger.info(
            f"Cycle {cycle_number} completed in {duration:.1f}s "
            f"(exploiter win rate: {win_rate:.2%})"
        )

        return exploiter_info

    def _get_checkpoint_iteration(self, checkpoint_path: Path, device: torch.device) -> int:
        """Extract iteration number from checkpoint.

        Args:
            checkpoint_path: Path to checkpoint file.
            device: Device for loading.

        Returns:
            Iteration number from checkpoint, or 0 if not found.
        """
        checkpoint = torch.load(checkpoint_path, map_location=device, weights_only=False)
        return checkpoint.get("iteration", 0)

    def _train_exploiter(
        self,
        target_checkpoint: Path,
        device: torch.device,
        cycle_number: int,
        training_config: dict[str, Any] | None = None,
    ) -> tuple[Path, float]:
        """Train an exploiter network against a frozen target.

        Args:
            target_checkpoint: Checkpoint of the target (main agent) to exploit.
            device: Device for training.
            cycle_number: Current cycle number for naming.
            training_config: Optional training configuration overrides.

        Returns:
            Tuple of (exploiter_checkpoint_path, win_rate_vs_target).
        """
        from _02_agents.neural.agent import NeuralAgent
        from _02_agents.neural.network import BeastyBarNetwork
        from _02_agents.neural.utils import load_network_from_checkpoint

        from .evaluation import compare_agents
        from .ppo import PPOConfig
        from .self_play import generate_games, trajectories_to_transitions

        # Load target network (frozen)
        target_network, target_config, _ = load_network_from_checkpoint(
            target_checkpoint, device=device
        )
        target_network.eval()
        target_agent = NeuralAgent(model=target_network, device=device, mode="greedy")

        # Create exploiter network (fresh or from target)
        if training_config and training_config.get("start_from_target", False):
            # Start exploiter from target weights
            exploiter_network, _, _ = load_network_from_checkpoint(
                target_checkpoint, device=device
            )
        else:
            # Fresh network with same architecture
            exploiter_network = BeastyBarNetwork(target_config).to(device)

        exploiter_network.train()

        # Setup optimizer
        ppo_config = PPOConfig()
        optimizer = torch.optim.Adam(
            exploiter_network.parameters(),
            lr=ppo_config.learning_rate,
        )

        # Training loop
        logger.info(
            f"Training exploiter for {self.config.exploiter_training_iterations} iterations"
        )

        for iteration in range(self.config.exploiter_training_iterations):
            # Generate games: exploiter (P0) vs frozen target (P1)
            trajectories = generate_games(
                network=exploiter_network,
                num_games=self.config.exploiter_games_per_iteration,
                temperature=1.0,
                device=device,
                opponent=target_agent,
            )

            # Collect transitions for exploiter (player 0 only)
            transitions = trajectories_to_transitions(trajectories, player=0)

            # PPO update on exploiter
            self._ppo_update_exploiter(
                network=exploiter_network,
                optimizer=optimizer,
                transitions=transitions,
                ppo_config=ppo_config,
                device=device,
            )

            # Log progress periodically
            if (iteration + 1) % 10 == 0:
                wins = sum(1 for t in trajectories if t.winner == 0)
                win_rate = wins / len(trajectories)
                logger.debug(f"Exploiter iteration {iteration + 1}: win rate {win_rate:.2%}")

        # Evaluate final exploiter
        exploiter_network.eval()
        exploiter_agent = NeuralAgent(model=exploiter_network, device=device, mode="greedy")
        eval_result = compare_agents(
            agent1=exploiter_agent,
            agent2=target_agent,
            num_games=100,
            play_both_sides=True,
        )
        final_win_rate = eval_result["win_rate_1"]

        # Save exploiter checkpoint
        exploiter_path = self.checkpoints_dir / f"exploiter_{cycle_number:04d}.pt"
        torch.save(
            {
                "model_state_dict": exploiter_network.state_dict(),
                "config": target_config.to_dict() if hasattr(target_config, "to_dict") else {},
                "cycle_number": cycle_number,
                "target_checkpoint": str(target_checkpoint),
                "win_rate_vs_target": final_win_rate,
            },
            exploiter_path,
        )

        logger.info(f"Exploiter saved to {exploiter_path} (win rate: {final_win_rate:.2%})")

        return exploiter_path, final_win_rate

    def _ppo_update_exploiter(
        self,
        network: torch.nn.Module,
        optimizer: torch.optim.Optimizer,
        transitions: list,
        ppo_config: Any,
        device: torch.device,
    ) -> dict[str, float]:
        """Perform PPO update on exploiter network.

        Args:
            network: Exploiter network to update.
            optimizer: Optimizer for the network.
            transitions: List of transitions from self-play.
            ppo_config: PPO configuration.
            device: Device for tensors.

        Returns:
            Dictionary of training metrics.
        """
        import numpy as np

        from .ppo import PPOBatch, compute_gae, iterate_minibatches
        from .ppo import entropy_bonus as compute_entropy
        from .ppo import policy_loss as compute_policy_loss
        from .ppo import value_loss as compute_value_loss

        if not transitions:
            return {}

        # Convert transitions to arrays
        observations = np.array([t.observation for t in transitions], dtype=np.float32)
        actions = np.array([t.action for t in transitions], dtype=np.int64)
        action_probs = np.array([t.action_prob for t in transitions], dtype=np.float32)
        values = np.array([t.value for t in transitions], dtype=np.float32)
        action_masks = np.array([t.action_mask for t in transitions], dtype=np.float32)
        rewards = np.array([t.reward for t in transitions], dtype=np.float32)
        dones = np.array([t.done for t in transitions], dtype=np.float32)

        # Compute GAE
        values_bootstrap = np.append(values, 0.0)
        advantages, returns = compute_gae(
            rewards=rewards,
            values=values_bootstrap,
            dones=dones,
            gamma=ppo_config.gamma,
            gae_lambda=ppo_config.gae_lambda,
        )

        # Create PPO batch
        ppo_batch = PPOBatch(
            observations=torch.from_numpy(observations).float().to(device),
            actions=torch.from_numpy(actions).long().to(device),
            old_log_probs=torch.log(
                torch.from_numpy(action_probs).float().clamp(min=1e-8)
            ).to(device),
            old_values=torch.from_numpy(values).float().to(device),
            action_masks=torch.from_numpy(action_masks).float().to(device),
            advantages=torch.from_numpy(advantages).float().to(device),
            returns=torch.from_numpy(returns).float().to(device),
        )

        # PPO epochs
        network.train()
        total_loss = 0.0
        num_updates = 0

        for _epoch in range(ppo_config.ppo_epochs):
            for minibatch in iterate_minibatches(
                ppo_batch, ppo_config.minibatch_size, shuffle=True
            ):
                optimizer.zero_grad()

                # Forward pass
                policy_logits, values_pred = network(minibatch.observations)
                values_pred = values_pred.squeeze(-1)

                # Normalize advantages
                advantages_norm = minibatch.advantages
                if ppo_config.normalize_advantages:
                    advantages_norm = (advantages_norm - advantages_norm.mean()) / (
                        advantages_norm.std() + 1e-8
                    )

                # Compute losses
                p_loss, _, _ = compute_policy_loss(
                    logits=policy_logits,
                    actions=minibatch.actions,
                    old_log_probs=minibatch.old_log_probs,
                    advantages=advantages_norm,
                    action_masks=minibatch.action_masks,
                    clip_epsilon=ppo_config.clip_epsilon,
                )

                v_loss = compute_value_loss(
                    predicted_values=values_pred,
                    returns=minibatch.returns,
                    old_values=minibatch.old_values,
                    clip_epsilon=ppo_config.clip_epsilon,
                    clip_value=ppo_config.clip_value,
                )

                entropy = compute_entropy(
                    logits=policy_logits,
                    action_masks=minibatch.action_masks,
                )

                # Combined loss
                loss = (
                    p_loss
                    + ppo_config.value_coef * v_loss
                    - ppo_config.entropy_coef * entropy
                )

                loss.backward()

                if ppo_config.max_grad_norm > 0:
                    torch.nn.utils.clip_grad_norm_(
                        network.parameters(),
                        ppo_config.max_grad_norm,
                    )

                optimizer.step()

                total_loss += loss.item()
                num_updates += 1

        return {"loss": total_loss / max(num_updates, 1)}

    def _add_exploiter(self, exploiter_info: ExploiterInfo) -> None:
        """Add exploiter to the pool, enforcing max_exploiters limit.

        Args:
            exploiter_info: Information about the exploiter to add.
        """
        self.exploiters.append(exploiter_info)

        # Remove oldest exploiters if over limit
        while len(self.exploiters) > self.config.max_exploiters:
            removed = self.exploiters.pop(0)
            logger.debug(
                f"Removed oldest exploiter (trained at iter {removed.trained_at_iteration})"
            )

    def get_exploiter_opponents(self) -> list[tuple[Path, float]]:
        """Get exploiter checkpoints and their weights for opponent pool.

        Returns:
            List of (checkpoint_path, weight) tuples for each exploiter.
            Weights are normalized to sum to exploiter_weight.
        """
        if not self.exploiters:
            return []

        # Distribute exploiter_weight equally among all exploiters
        weight_per_exploiter = self.config.exploiter_weight / len(self.exploiters)

        return [
            (exp.checkpoint_path, weight_per_exploiter)
            for exp in self.exploiters
        ]

    def get_statistics(self) -> dict[str, Any]:
        """Get statistics about the exploit-patch manager.

        Returns:
            Dictionary with manager statistics.
        """
        return {
            "num_exploiters": len(self.exploiters),
            "num_cycles": len(self.cycle_history),
            "win_rate_records": len(self._win_rate_records),
            "last_cycle_iteration": self._last_cycle_iteration,
            "avg_exploiter_win_rate": (
                np.mean([e.win_rate_vs_target for e in self.exploiters])
                if self.exploiters
                else 0.0
            ),
            "plateau_triggers": sum(1 for c in self.cycle_history if c.trigger == "plateau"),
            "scheduled_triggers": sum(1 for c in self.cycle_history if c.trigger == "scheduled"),
        }

    def to_dict(self) -> dict[str, Any]:
        """Serialize manager state for checkpointing.

        Returns:
            Dictionary representation of the manager state.
        """
        return {
            "config": self.config.to_dict(),
            "exploiters": [e.to_dict() for e in self.exploiters],
            "cycle_history": [c.to_dict() for c in self.cycle_history],
            "win_rate_records": [
                {"iteration": r.iteration, "win_rate": r.win_rate}
                for r in self._win_rate_records
            ],
            "last_cycle_iteration": self._last_cycle_iteration,
            "checkpoints_dir": str(self.checkpoints_dir),
        }

    @classmethod
    def from_dict(cls, data: dict[str, Any]) -> ExploitPatchManager:
        """Deserialize manager state from checkpoint.

        Args:
            data: Dictionary representation of manager state.

        Returns:
            ExploitPatchManager instance with restored state.
        """
        config = CycleConfig.from_dict(data.get("config", {}))
        checkpoints_dir = Path(data.get("checkpoints_dir", "checkpoints"))

        manager = cls(config=config, checkpoints_dir=checkpoints_dir.parent)

        # Restore exploiters
        manager.exploiters = [
            ExploiterInfo.from_dict(e) for e in data.get("exploiters", [])
        ]

        # Restore cycle history
        manager.cycle_history = [
            CycleRecord.from_dict(c) for c in data.get("cycle_history", [])
        ]

        # Restore win rate records
        manager._win_rate_records = [
            WinRateRecord(iteration=r["iteration"], win_rate=r["win_rate"])
            for r in data.get("win_rate_records", [])
        ]

        # Restore last cycle iteration
        manager._last_cycle_iteration = data.get(
            "last_cycle_iteration", -config.cycle_interval
        )

        logger.info(
            f"Restored ExploitPatchManager: "
            f"{len(manager.exploiters)} exploiters, "
            f"{len(manager.cycle_history)} cycles"
        )

        return manager


__all__ = [
    "CycleConfig",
    "CycleRecord",
    "ExploitPatchManager",
    "ExploiterInfo",
]
